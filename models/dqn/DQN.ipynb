{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from preprocessing.create_dummy import create_dummy\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df = pd.read_csv('../data/filled.csv')\n",
    "data_to_model = create_dummy(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data_to_model = shuffle(data_to_model, random_state=0)\n",
    "X = data_to_model.drop(columns=\"BAD\")\n",
    "y = data_to_model['BAD']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "for col in X.columns:\n",
    "    min_ = X[col].min()\n",
    "    max_ = X[col].max()\n",
    "    X[col] -= min_\n",
    "    X[col] /= max_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "          LOAN   MORTDUE     VALUE       YOJ  DEROG    DELINQ     CLAGE  \\\n916   0.087875  0.206405  0.112161  0.073171    0.0  0.000000  0.043370   \n3338  0.184650  0.177319  0.104353  0.024390    0.0  0.000000  0.070532   \n5754  0.473860  0.179446  0.047359  0.217616    0.6  0.000000  0.081936   \n4290  0.236930  0.240708  0.149036  0.217616    0.0  0.000000  0.166304   \n1540  0.113459  0.110267  0.052472  0.463415    0.0  0.000000  0.196400   \n...        ...       ...       ...       ...    ...       ...       ...   \n4931  0.276974  0.552166  0.325132  0.146341    0.0  0.266667  0.415966   \n3264  0.181313  0.279882  0.148551  0.390244    0.0  0.000000  0.149030   \n1653  0.119021  0.139399  0.067221  0.268293    0.0  0.000000  0.092849   \n2607  0.154616  0.192559  0.116835  0.009756    0.0  0.000000  0.329985   \n2732  0.159066  0.250076  0.160210  0.073171    0.0  0.000000  0.090352   \n\n          NINQ      CLNO   DEBTINC  REASON_DebtCon  REASON_HomeImp  JOB_Mgr  \\\n916   0.058824  0.042254  0.163568             1.0             0.0      0.0   \n3338  0.000000  0.239437  0.122003             1.0             0.0      1.0   \n5754  0.069768  0.267606  0.130128             1.0             0.0      0.0   \n4290  0.000000  0.281690  0.141896             1.0             0.0      0.0   \n1540  0.058824  0.239437  0.176135             0.0             1.0      1.0   \n...        ...       ...       ...             ...             ...      ...   \n4931  0.069768  0.774648  0.160910             0.0             1.0      0.0   \n3264  0.058824  0.239437  0.182523             1.0             0.0      0.0   \n1653  0.058824  0.253521  0.158667             1.0             0.0      1.0   \n2607  0.176471  0.436620  0.163568             0.0             1.0      0.0   \n2732  0.176471  0.211268  0.158254             1.0             0.0      0.0   \n\n      JOB_Office  JOB_Other  JOB_ProfExe  JOB_Sales  JOB_Self  \n916          0.0        0.0          1.0        0.0       0.0  \n3338         0.0        0.0          0.0        0.0       0.0  \n5754         0.0        1.0          0.0        0.0       0.0  \n4290         0.0        0.0          1.0        0.0       0.0  \n1540         0.0        0.0          0.0        0.0       0.0  \n...          ...        ...          ...        ...       ...  \n4931         0.0        0.0          1.0        0.0       0.0  \n3264         0.0        1.0          0.0        0.0       0.0  \n1653         0.0        0.0          0.0        0.0       0.0  \n2607         0.0        1.0          0.0        0.0       0.0  \n2732         1.0        0.0          0.0        0.0       0.0  \n\n[5960 rows x 18 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LOAN</th>\n      <th>MORTDUE</th>\n      <th>VALUE</th>\n      <th>YOJ</th>\n      <th>DEROG</th>\n      <th>DELINQ</th>\n      <th>CLAGE</th>\n      <th>NINQ</th>\n      <th>CLNO</th>\n      <th>DEBTINC</th>\n      <th>REASON_DebtCon</th>\n      <th>REASON_HomeImp</th>\n      <th>JOB_Mgr</th>\n      <th>JOB_Office</th>\n      <th>JOB_Other</th>\n      <th>JOB_ProfExe</th>\n      <th>JOB_Sales</th>\n      <th>JOB_Self</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>916</th>\n      <td>0.087875</td>\n      <td>0.206405</td>\n      <td>0.112161</td>\n      <td>0.073171</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.043370</td>\n      <td>0.058824</td>\n      <td>0.042254</td>\n      <td>0.163568</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3338</th>\n      <td>0.184650</td>\n      <td>0.177319</td>\n      <td>0.104353</td>\n      <td>0.024390</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.070532</td>\n      <td>0.000000</td>\n      <td>0.239437</td>\n      <td>0.122003</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5754</th>\n      <td>0.473860</td>\n      <td>0.179446</td>\n      <td>0.047359</td>\n      <td>0.217616</td>\n      <td>0.6</td>\n      <td>0.000000</td>\n      <td>0.081936</td>\n      <td>0.069768</td>\n      <td>0.267606</td>\n      <td>0.130128</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4290</th>\n      <td>0.236930</td>\n      <td>0.240708</td>\n      <td>0.149036</td>\n      <td>0.217616</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.166304</td>\n      <td>0.000000</td>\n      <td>0.281690</td>\n      <td>0.141896</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1540</th>\n      <td>0.113459</td>\n      <td>0.110267</td>\n      <td>0.052472</td>\n      <td>0.463415</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.196400</td>\n      <td>0.058824</td>\n      <td>0.239437</td>\n      <td>0.176135</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4931</th>\n      <td>0.276974</td>\n      <td>0.552166</td>\n      <td>0.325132</td>\n      <td>0.146341</td>\n      <td>0.0</td>\n      <td>0.266667</td>\n      <td>0.415966</td>\n      <td>0.069768</td>\n      <td>0.774648</td>\n      <td>0.160910</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3264</th>\n      <td>0.181313</td>\n      <td>0.279882</td>\n      <td>0.148551</td>\n      <td>0.390244</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.149030</td>\n      <td>0.058824</td>\n      <td>0.239437</td>\n      <td>0.182523</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1653</th>\n      <td>0.119021</td>\n      <td>0.139399</td>\n      <td>0.067221</td>\n      <td>0.268293</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.092849</td>\n      <td>0.058824</td>\n      <td>0.253521</td>\n      <td>0.158667</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2607</th>\n      <td>0.154616</td>\n      <td>0.192559</td>\n      <td>0.116835</td>\n      <td>0.009756</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.329985</td>\n      <td>0.176471</td>\n      <td>0.436620</td>\n      <td>0.163568</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2732</th>\n      <td>0.159066</td>\n      <td>0.250076</td>\n      <td>0.160210</td>\n      <td>0.073171</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.090352</td>\n      <td>0.176471</td>\n      <td>0.211268</td>\n      <td>0.158254</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5960 rows Ã— 18 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import random\n",
    "# actions GOOD BAD\n",
    "GOOD = 0\n",
    "BAD = 1\n",
    "\n",
    "class EnvironmentSimulator:\n",
    "    def __init__(self, labels=y):\n",
    "        self.at_eq_yt = 1  # payout for a_t == y_t\n",
    "        self.at_ne_yt = -1  # payout for a_t != y_t\n",
    "        self.y_t = labels.tolist()\n",
    "        self.n = len(self.y_t)\n",
    "\n",
    "    def take_action(self, state, action):\n",
    "        # compare a_t with actual label y_t\n",
    "        if action == BAD and self.y_t[state] == 1:\n",
    "            reward = 1\n",
    "        elif action == GOOD and self.y_t[state] == 0:\n",
    "            reward = 1/4\n",
    "        elif action == GOOD and self.y_t[state] == 1:  # a_t != y_t\n",
    "            reward = -1\n",
    "        else: # action == BAD and self.y_t[state] == 0\n",
    "            reward = -1/4\n",
    "\n",
    "        return reward\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\envs\\snakes\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, learning_rate=0.5, discount=0.95, exploration_rate=1.0, iterations=10000, data=X):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount # How much we appreciate future reward over current\n",
    "        self.exploration_rate = exploration_rate # Initial exploration rate\n",
    "        self.exploration_delta = 1.0 / iterations # Shift from exploration to explotation\n",
    "\n",
    "        # Input has 12 neuron representing new client in our state\n",
    "        self.input_count = 18\n",
    "        # Output is two neurons, each represents Q-value for action (GOOD and BAD)\n",
    "        self.output_count = 2\n",
    "        self.data = data\n",
    "\n",
    "        self.session = tf.Session()\n",
    "        self.define_model()\n",
    "        self.session.run(self.initializer)\n",
    "\n",
    "    # Define tensorflow model graph\n",
    "    def define_model(self):\n",
    "        # Input is an array of single item (state)\n",
    "        self.model_input = tf.placeholder(dtype=tf.float32, shape=[None, self.input_count])\n",
    "\n",
    "        # 32 hidden neurons per layer\n",
    "        layer_size = 1024\n",
    "        # Two hidden layers of 32 neurons with sigmoid activation initialized to zero for stability\n",
    "        fc1 = tf.layers.dense(self.model_input, layer_size, activation='relu', kernel_initializer='glorot_uniform')\n",
    "        fc2 = tf.layers.dense(fc1, layer_size, activation='relu', kernel_initializer='glorot_uniform')\n",
    "        fc3 = tf.layers.dense(fc2, layer_size, activation='relu', kernel_initializer='glorot_uniform')\n",
    "        fc4 = tf.layers.dense(fc3, layer_size, activation='relu', kernel_initializer='glorot_uniform')\n",
    "\n",
    "        # Output is two values, Q for both possible actions GOOD and BAD\n",
    "        self.model_output = tf.layers.dense(fc4, self.output_count)\n",
    "\n",
    "        # This is for feeding training output (a.k.a ideal target values)\n",
    "        self.target_output = tf.placeholder(shape=[None, self.output_count], dtype=tf.float32)\n",
    "        # Loss is mean squared difference between current output and ideal target values\n",
    "        loss = tf.losses.compute_weighted_loss(self.target_output, self.model_output)\n",
    "        # Optimizer adjusts weights to minimize loss, with the speed of learning_rate\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "        # Initializer to set weights to initial values\n",
    "        self.initializer = tf.global_variables_initializer()\n",
    "\n",
    "    # Ask model to estimate Q value for specific state (inference)\n",
    "    def get_Q(self, state):\n",
    "        # Model input: Single state represented by array of single item (state)\n",
    "        # Model output: Array of Q values for single state\n",
    "        return self.session.run(self.model_output, feed_dict={self.model_input: self.data.iloc[state].values.reshape((-1, 18))})[0]\n",
    "\n",
    "    def get_next_action(self, state):\n",
    "        # if random.random() > self.exploration_rate:\n",
    "        return self.greedy_action(state)\n",
    "        # else:\n",
    "        #     return self.random_action()\n",
    "\n",
    "    # Which action (GOOD or BAD) has bigger Q-value, estimated by our model (inference).\n",
    "    def greedy_action(self, state):\n",
    "        # argmax picks the higher Q-value and returns the index (GOOD=0, BAD=1)\n",
    "        return np.argmax(self.get_Q(state))\n",
    "\n",
    "    def random_action(self):\n",
    "        return GOOD if random.random() < 0.5 else BAD\n",
    "\n",
    "    def train(self, old_state, action, reward, new_state):\n",
    "        # Ask the model for the Q values of the old state (inference)\n",
    "        old_state_Q_values = self.get_Q(old_state)\n",
    "\n",
    "        # Ask the model for the Q values of the new state (inference)\n",
    "        new_state_Q_values = self.get_Q(new_state)\n",
    "\n",
    "        # Real Q value for the action we took. This is what we will train towards.\n",
    "        old_state_Q_values[action] = reward + self.discount * np.amax(new_state_Q_values)\n",
    "\n",
    "        # Setup training data\n",
    "        training_input = self.data.iloc[old_state].values.reshape((-1, 18))\n",
    "\n",
    "        target_output = [old_state_Q_values]\n",
    "        training_data = {self.model_input: training_input, self.target_output: target_output}\n",
    "\n",
    "        # Train\n",
    "        self.session.run(self.optimizer, feed_dict=training_data)\n",
    "\n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        # Train our model with new data\n",
    "        self.train(old_state, action, reward, new_state)\n",
    "\n",
    "        # Finally shift our exploration_rate toward zero (less gambling)\n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate -= self.exploration_delta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-b53e30d8f298>:34: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\envs\\snakes\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "{\"step\": 0, \"performance\": 0.001, \"total_reward\": 0.25}\n",
      "{\"step\": 250, \"performance\": -0.027, \"total_reward\": -6.5}\n",
      "{\"step\": 500, \"performance\": -0.049, \"total_reward\": -18.75}\n",
      "{\"step\": 750, \"performance\": -0.029, \"total_reward\": -26.0}\n",
      "{\"step\": 1000, \"performance\": 0.019, \"total_reward\": -21.25}\n",
      "{\"step\": 1250, \"performance\": -0.032, \"total_reward\": -29.25}\n",
      "{\"step\": 1500, \"performance\": -0.045, \"total_reward\": -40.5}\n",
      "{\"step\": 1750, \"performance\": -0.104, \"total_reward\": -66.5}\n",
      "{\"step\": 2000, \"performance\": -0.101, \"total_reward\": -91.75}\n",
      "{\"step\": 2250, \"performance\": -0.124, \"total_reward\": -122.75}\n",
      "{\"step\": 2500, \"performance\": -0.141, \"total_reward\": -158.0}\n",
      "{\"step\": 2750, \"performance\": -0.083, \"total_reward\": -178.75}\n",
      "{\"step\": 3000, \"performance\": -0.027, \"total_reward\": -185.5}\n",
      "{\"step\": 3250, \"performance\": -0.029, \"total_reward\": -192.75}\n",
      "{\"step\": 3500, \"performance\": -0.076, \"total_reward\": -211.75}\n",
      "{\"step\": 3750, \"performance\": -0.076, \"total_reward\": -230.75}\n",
      "{\"step\": 4000, \"performance\": -0.086, \"total_reward\": -252.25}\n",
      "{\"step\": 4250, \"performance\": -0.041, \"total_reward\": -262.5}\n",
      "{\"step\": 4500, \"performance\": -0.064, \"total_reward\": -278.5}\n",
      "{\"step\": 4750, \"performance\": -0.089, \"total_reward\": -300.75}\n",
      "{\"step\": 5000, \"performance\": -0.084, \"total_reward\": -321.75}\n",
      "{\"step\": 5250, \"performance\": -0.083, \"total_reward\": -342.5}\n",
      "{\"step\": 5500, \"performance\": -0.001, \"total_reward\": -342.75}\n",
      "{\"step\": 5750, \"performance\": -0.061, \"total_reward\": -358.0}\n",
      "{\"step\": 6000, \"performance\": -0.092, \"total_reward\": -381.0}\n",
      "{\"step\": 6250, \"performance\": -0.034, \"total_reward\": -389.5}\n",
      "{\"step\": 6500, \"performance\": -0.061, \"total_reward\": -404.75}\n",
      "{\"step\": 6750, \"performance\": -0.073, \"total_reward\": -423.0}\n",
      "{\"step\": 7000, \"performance\": -0.064, \"total_reward\": -439.0}\n",
      "{\"step\": 7250, \"performance\": -0.04, \"total_reward\": -449.0}\n",
      "{\"step\": 7500, \"performance\": -0.084, \"total_reward\": -470.0}\n",
      "{\"step\": 7750, \"performance\": -0.024, \"total_reward\": -476.0}\n",
      "{\"step\": 8000, \"performance\": -0.022, \"total_reward\": -481.5}\n",
      "{\"step\": 8250, \"performance\": -0.045, \"total_reward\": -492.75}\n",
      "{\"step\": 8500, \"performance\": -0.092, \"total_reward\": -515.75}\n",
      "{\"step\": 8750, \"performance\": -0.103, \"total_reward\": -541.5}\n",
      "{\"step\": 9000, \"performance\": -0.08, \"total_reward\": -561.5}\n",
      "{\"step\": 9250, \"performance\": -0.028, \"total_reward\": -568.5}\n",
      "{\"step\": 9500, \"performance\": 0.028, \"total_reward\": -561.5}\n",
      "{\"step\": 9750, \"performance\": -0.082, \"total_reward\": -582.0}\n",
      "{\"step\": 10000, \"performance\": -0.065, \"total_reward\": -598.25}\n",
      "{\"step\": 10250, \"performance\": -0.079, \"total_reward\": -618.0}\n",
      "{\"step\": 10500, \"performance\": -0.051, \"total_reward\": -630.75}\n",
      "{\"step\": 10750, \"performance\": -0.11, \"total_reward\": -658.25}\n",
      "{\"step\": 11000, \"performance\": -0.048, \"total_reward\": -670.25}\n",
      "{\"step\": 11250, \"performance\": -0.077, \"total_reward\": -689.5}\n",
      "{\"step\": 11500, \"performance\": -0.097, \"total_reward\": -713.75}\n",
      "{\"step\": 11750, \"performance\": 0.006, \"total_reward\": -712.25}\n",
      "{\"step\": 12000, \"performance\": -0.044, \"total_reward\": -723.25}\n",
      "{\"step\": 12250, \"performance\": -0.051, \"total_reward\": -736.0}\n",
      "{\"step\": 12500, \"performance\": -0.042, \"total_reward\": -746.5}\n",
      "{\"step\": 12750, \"performance\": -0.015, \"total_reward\": -750.25}\n",
      "{\"step\": 13000, \"performance\": -0.054, \"total_reward\": -763.75}\n",
      "{\"step\": 13250, \"performance\": -0.103, \"total_reward\": -789.5}\n",
      "{\"step\": 13500, \"performance\": -0.071, \"total_reward\": -807.25}\n",
      "{\"step\": 13750, \"performance\": -0.027, \"total_reward\": -814.0}\n",
      "{\"step\": 14000, \"performance\": -0.06, \"total_reward\": -829.0}\n",
      "{\"step\": 14250, \"performance\": -0.056, \"total_reward\": -843.0}\n",
      "{\"step\": 14500, \"performance\": -0.048, \"total_reward\": -855.0}\n",
      "{\"step\": 14750, \"performance\": -0.048, \"total_reward\": -867.0}\n",
      "{\"step\": 15000, \"performance\": -0.034, \"total_reward\": -875.5}\n",
      "{\"step\": 15250, \"performance\": -0.077, \"total_reward\": -894.75}\n",
      "{\"step\": 15500, \"performance\": -0.076, \"total_reward\": -913.75}\n",
      "{\"step\": 15750, \"performance\": -0.005, \"total_reward\": -915.0}\n",
      "{\"step\": 16000, \"performance\": 0.004, \"total_reward\": -914.0}\n",
      "{\"step\": 16250, \"performance\": 0.022, \"total_reward\": -908.5}\n",
      "{\"step\": 16500, \"performance\": -0.02, \"total_reward\": -913.5}\n",
      "{\"step\": 16750, \"performance\": -0.06, \"total_reward\": -928.5}\n",
      "{\"step\": 17000, \"performance\": -0.054, \"total_reward\": -942.0}\n",
      "{\"step\": 17250, \"performance\": -0.117, \"total_reward\": -971.25}\n",
      "{\"step\": 17500, \"performance\": -0.039, \"total_reward\": -981.0}\n",
      "{\"step\": 17750, \"performance\": -0.085, \"total_reward\": -1002.25}\n",
      "{\"step\": 18000, \"performance\": -0.132, \"total_reward\": -1035.25}\n",
      "{\"step\": 18250, \"performance\": -0.102, \"total_reward\": -1060.75}\n",
      "{\"step\": 18500, \"performance\": -0.065, \"total_reward\": -1077.0}\n",
      "{\"step\": 18750, \"performance\": -0.093, \"total_reward\": -1100.25}\n",
      "{\"step\": 19000, \"performance\": -0.091, \"total_reward\": -1123.0}\n",
      "{\"step\": 19250, \"performance\": -0.091, \"total_reward\": -1145.75}\n",
      "{\"step\": 19500, \"performance\": -0.122, \"total_reward\": -1176.25}\n",
      "{\"step\": 19750, \"performance\": -0.076, \"total_reward\": -1195.25}\n",
      "{\"step\": 20000, \"performance\": -0.077, \"total_reward\": -1214.5}\n",
      "{\"step\": 20250, \"performance\": -0.05, \"total_reward\": -1227.0}\n",
      "{\"step\": 20500, \"performance\": -0.054, \"total_reward\": -1240.5}\n",
      "{\"step\": 20750, \"performance\": -0.046, \"total_reward\": -1252.0}\n",
      "{\"step\": 21000, \"performance\": 0.051, \"total_reward\": -1239.25}\n",
      "{\"step\": 21250, \"performance\": -0.048, \"total_reward\": -1251.25}\n",
      "{\"step\": 21500, \"performance\": -0.071, \"total_reward\": -1269.0}\n",
      "{\"step\": 21750, \"performance\": -0.096, \"total_reward\": -1293.0}\n",
      "{\"step\": 22000, \"performance\": -0.022, \"total_reward\": -1298.5}\n",
      "{\"step\": 22250, \"performance\": -0.037, \"total_reward\": -1307.75}\n",
      "{\"step\": 22500, \"performance\": -0.049, \"total_reward\": -1320.0}\n",
      "{\"step\": 22750, \"performance\": 0.002, \"total_reward\": -1319.5}\n",
      "{\"step\": 23000, \"performance\": -0.045, \"total_reward\": -1330.75}\n",
      "{\"step\": 23250, \"performance\": -0.056, \"total_reward\": -1344.75}\n",
      "{\"step\": 23500, \"performance\": -0.045, \"total_reward\": -1356.0}\n",
      "{\"step\": 23750, \"performance\": -0.014, \"total_reward\": -1359.5}\n",
      "{\"step\": 24000, \"performance\": -0.083, \"total_reward\": -1380.25}\n",
      "{\"step\": 24250, \"performance\": 0.014, \"total_reward\": -1376.75}\n",
      "{\"step\": 24500, \"performance\": -0.057, \"total_reward\": -1391.0}\n",
      "{\"step\": 24750, \"performance\": -0.006, \"total_reward\": -1392.5}\n",
      "{\"step\": 25000, \"performance\": -0.041, \"total_reward\": -1402.75}\n",
      "{\"step\": 25250, \"performance\": -0.025, \"total_reward\": -1409.0}\n",
      "{\"step\": 25500, \"performance\": -0.035, \"total_reward\": -1417.75}\n",
      "{\"step\": 25750, \"performance\": -0.062, \"total_reward\": -1433.25}\n",
      "{\"step\": 26000, \"performance\": -0.081, \"total_reward\": -1453.5}\n",
      "{\"step\": 26250, \"performance\": 0.04, \"total_reward\": -1443.5}\n",
      "{\"step\": 26500, \"performance\": -0.007, \"total_reward\": -1445.25}\n",
      "{\"step\": 26750, \"performance\": -0.109, \"total_reward\": -1472.5}\n",
      "{\"step\": 27000, \"performance\": -0.063, \"total_reward\": -1488.25}\n",
      "{\"step\": 27250, \"performance\": -0.021, \"total_reward\": -1493.5}\n",
      "{\"step\": 27500, \"performance\": -0.046, \"total_reward\": -1505.0}\n",
      "{\"step\": 27750, \"performance\": -0.063, \"total_reward\": -1520.75}\n",
      "{\"step\": 28000, \"performance\": -0.032, \"total_reward\": -1528.75}\n",
      "{\"step\": 28250, \"performance\": -0.034, \"total_reward\": -1537.25}\n",
      "{\"step\": 28500, \"performance\": -0.1, \"total_reward\": -1562.25}\n",
      "{\"step\": 28750, \"performance\": -0.022, \"total_reward\": -1567.75}\n",
      "{\"step\": 29000, \"performance\": -0.033, \"total_reward\": -1576.0}\n",
      "{\"step\": 29250, \"performance\": -0.048, \"total_reward\": -1588.0}\n",
      "{\"step\": 29500, \"performance\": -0.082, \"total_reward\": -1608.5}\n",
      "{\"step\": 29750, \"performance\": -0.047, \"total_reward\": -1620.25}\n",
      "{\"step\": 30000, \"performance\": -0.008, \"total_reward\": -1622.25}\n",
      "{\"step\": 30250, \"performance\": -0.077, \"total_reward\": -1641.5}\n",
      "{\"step\": 30500, \"performance\": -0.017, \"total_reward\": -1645.75}\n",
      "{\"step\": 30750, \"performance\": -0.022, \"total_reward\": -1651.25}\n",
      "{\"step\": 31000, \"performance\": -0.037, \"total_reward\": -1660.5}\n",
      "{\"step\": 31250, \"performance\": -0.112, \"total_reward\": -1688.5}\n",
      "{\"step\": 31500, \"performance\": -0.009, \"total_reward\": -1690.75}\n",
      "{\"step\": 31750, \"performance\": -0.022, \"total_reward\": -1696.25}\n",
      "{\"step\": 32000, \"performance\": -0.056, \"total_reward\": -1710.25}\n",
      "{\"step\": 32250, \"performance\": -0.054, \"total_reward\": -1723.75}\n",
      "{\"step\": 32500, \"performance\": -0.064, \"total_reward\": -1739.75}\n",
      "{\"step\": 32750, \"performance\": -0.013, \"total_reward\": -1743.0}\n",
      "{\"step\": 33000, \"performance\": -0.078, \"total_reward\": -1762.5}\n",
      "{\"step\": 33250, \"performance\": -0.03, \"total_reward\": -1770.0}\n",
      "{\"step\": 33500, \"performance\": 0.001, \"total_reward\": -1769.75}\n",
      "{\"step\": 33750, \"performance\": -0.038, \"total_reward\": -1779.25}\n",
      "{\"step\": 34000, \"performance\": -0.008, \"total_reward\": -1781.25}\n",
      "{\"step\": 34250, \"performance\": 0.01, \"total_reward\": -1778.75}\n",
      "{\"step\": 34500, \"performance\": 0.023, \"total_reward\": -1773.0}\n",
      "{\"step\": 34750, \"performance\": 0.023, \"total_reward\": -1767.25}\n",
      "{\"step\": 35000, \"performance\": 0.004, \"total_reward\": -1766.25}\n",
      "{\"step\": 35250, \"performance\": -0.048, \"total_reward\": -1778.25}\n",
      "{\"step\": 35500, \"performance\": -0.061, \"total_reward\": -1793.5}\n",
      "{\"step\": 35750, \"performance\": 0.006, \"total_reward\": -1792.0}\n",
      "{\"step\": 36000, \"performance\": -0.086, \"total_reward\": -1813.5}\n",
      "{\"step\": 36250, \"performance\": -0.042, \"total_reward\": -1824.0}\n",
      "{\"step\": 36500, \"performance\": -0.064, \"total_reward\": -1840.0}\n",
      "{\"step\": 36750, \"performance\": -0.038, \"total_reward\": -1849.5}\n",
      "{\"step\": 37000, \"performance\": -0.039, \"total_reward\": -1859.25}\n",
      "{\"step\": 37250, \"performance\": -0.008, \"total_reward\": -1861.25}\n",
      "{\"step\": 37500, \"performance\": -0.031, \"total_reward\": -1869.0}\n",
      "{\"step\": 37750, \"performance\": -0.051, \"total_reward\": -1881.75}\n",
      "{\"step\": 38000, \"performance\": -0.053, \"total_reward\": -1895.0}\n",
      "{\"step\": 38250, \"performance\": -0.108, \"total_reward\": -1922.0}\n",
      "{\"step\": 38500, \"performance\": -0.035, \"total_reward\": -1930.75}\n",
      "{\"step\": 38750, \"performance\": 0.005, \"total_reward\": -1929.5}\n",
      "{\"step\": 39000, \"performance\": -0.02, \"total_reward\": -1934.5}\n",
      "{\"step\": 39250, \"performance\": -0.023, \"total_reward\": -1940.25}\n",
      "{\"step\": 39500, \"performance\": -0.061, \"total_reward\": -1955.5}\n",
      "{\"step\": 39750, \"performance\": -0.014, \"total_reward\": -1959.0}\n",
      "{\"step\": 40000, \"performance\": -0.053, \"total_reward\": -1972.25}\n",
      "{\"step\": 40250, \"performance\": -0.038, \"total_reward\": -1981.75}\n",
      "{\"step\": 40500, \"performance\": 0.011, \"total_reward\": -1979.0}\n",
      "{\"step\": 40750, \"performance\": -0.015, \"total_reward\": -1982.75}\n",
      "{\"step\": 41000, \"performance\": 0.0, \"total_reward\": -1982.75}\n",
      "{\"step\": 41250, \"performance\": 0.034, \"total_reward\": -1974.25}\n",
      "{\"step\": 41500, \"performance\": -0.029, \"total_reward\": -1981.5}\n",
      "{\"step\": 41750, \"performance\": -0.047, \"total_reward\": -1993.25}\n",
      "{\"step\": 42000, \"performance\": -0.041, \"total_reward\": -2003.5}\n",
      "{\"step\": 42250, \"performance\": -0.047, \"total_reward\": -2015.25}\n",
      "{\"step\": 42500, \"performance\": -0.028, \"total_reward\": -2022.25}\n",
      "{\"step\": 42750, \"performance\": -0.044, \"total_reward\": -2033.25}\n",
      "{\"step\": 43000, \"performance\": 0.006, \"total_reward\": -2031.75}\n",
      "{\"step\": 43250, \"performance\": -0.039, \"total_reward\": -2041.5}\n",
      "{\"step\": 43500, \"performance\": -0.072, \"total_reward\": -2059.5}\n",
      "{\"step\": 43750, \"performance\": -0.092, \"total_reward\": -2082.5}\n",
      "{\"step\": 44000, \"performance\": 0.072, \"total_reward\": -2064.5}\n",
      "{\"step\": 44250, \"performance\": -0.018, \"total_reward\": -2069.0}\n",
      "{\"step\": 44500, \"performance\": -0.077, \"total_reward\": -2088.25}\n",
      "{\"step\": 44750, \"performance\": -0.057, \"total_reward\": -2102.5}\n",
      "{\"step\": 45000, \"performance\": -0.038, \"total_reward\": -2112.0}\n",
      "{\"step\": 45250, \"performance\": -0.012, \"total_reward\": -2115.0}\n",
      "{\"step\": 45500, \"performance\": -0.05, \"total_reward\": -2127.5}\n",
      "{\"step\": 45750, \"performance\": -0.029, \"total_reward\": -2134.75}\n",
      "{\"step\": 46000, \"performance\": -0.11, \"total_reward\": -2162.25}\n",
      "{\"step\": 46250, \"performance\": 0.003, \"total_reward\": -2161.5}\n",
      "{\"step\": 46500, \"performance\": -0.003, \"total_reward\": -2162.25}\n",
      "{\"step\": 46750, \"performance\": 0.019, \"total_reward\": -2157.5}\n",
      "{\"step\": 47000, \"performance\": -0.007, \"total_reward\": -2159.25}\n",
      "{\"step\": 47250, \"performance\": -0.038, \"total_reward\": -2168.75}\n",
      "{\"step\": 47500, \"performance\": -0.036, \"total_reward\": -2177.75}\n",
      "{\"step\": 47750, \"performance\": -0.118, \"total_reward\": -2207.25}\n",
      "{\"step\": 48000, \"performance\": -0.045, \"total_reward\": -2218.5}\n",
      "{\"step\": 48250, \"performance\": -0.06, \"total_reward\": -2233.5}\n",
      "{\"step\": 48500, \"performance\": -0.041, \"total_reward\": -2243.75}\n",
      "{\"step\": 48750, \"performance\": -0.082, \"total_reward\": -2264.25}\n",
      "{\"step\": 49000, \"performance\": -0.042, \"total_reward\": -2274.75}\n",
      "{\"step\": 49250, \"performance\": -0.019, \"total_reward\": -2279.5}\n",
      "{\"step\": 49500, \"performance\": -0.045, \"total_reward\": -2290.75}\n",
      "{\"step\": 49750, \"performance\": -0.07, \"total_reward\": -2308.25}\n",
      "{\"step\": 50000, \"performance\": -0.049, \"total_reward\": -2320.5}\n",
      "{\"step\": 50250, \"performance\": -0.04, \"total_reward\": -2330.5}\n",
      "{\"step\": 50500, \"performance\": -0.018, \"total_reward\": -2335.0}\n",
      "{\"step\": 50750, \"performance\": 0.019, \"total_reward\": -2330.25}\n",
      "{\"step\": 51000, \"performance\": -0.027, \"total_reward\": -2337.0}\n",
      "{\"step\": 51250, \"performance\": -0.04, \"total_reward\": -2347.0}\n",
      "{\"step\": 51500, \"performance\": -0.008, \"total_reward\": -2349.0}\n",
      "{\"step\": 51750, \"performance\": -0.023, \"total_reward\": -2354.75}\n",
      "{\"step\": 52000, \"performance\": 0.022, \"total_reward\": -2349.25}\n",
      "{\"step\": 52250, \"performance\": 0.003, \"total_reward\": -2348.5}\n",
      "{\"step\": 52500, \"performance\": -0.006, \"total_reward\": -2350.0}\n",
      "{\"step\": 52750, \"performance\": -0.017, \"total_reward\": -2354.25}\n",
      "{\"step\": 53000, \"performance\": -0.017, \"total_reward\": -2358.5}\n",
      "{\"step\": 53250, \"performance\": 0.017, \"total_reward\": -2354.25}\n",
      "{\"step\": 53500, \"performance\": -0.049, \"total_reward\": -2366.5}\n",
      "{\"step\": 53750, \"performance\": -0.084, \"total_reward\": -2387.5}\n",
      "{\"step\": 54000, \"performance\": -0.071, \"total_reward\": -2405.25}\n",
      "{\"step\": 54250, \"performance\": -0.005, \"total_reward\": -2406.5}\n",
      "{\"step\": 54500, \"performance\": -0.034, \"total_reward\": -2415.0}\n",
      "{\"step\": 54750, \"performance\": -0.049, \"total_reward\": -2427.25}\n",
      "{\"step\": 55000, \"performance\": -0.073, \"total_reward\": -2445.5}\n",
      "{\"step\": 55250, \"performance\": -0.03, \"total_reward\": -2453.0}\n",
      "{\"step\": 55500, \"performance\": -0.047, \"total_reward\": -2464.75}\n",
      "{\"step\": 55750, \"performance\": 0.007, \"total_reward\": -2463.0}\n",
      "{\"step\": 56000, \"performance\": 0.014, \"total_reward\": -2459.5}\n",
      "{\"step\": 56250, \"performance\": 0.021, \"total_reward\": -2454.25}\n",
      "{\"step\": 56500, \"performance\": -0.041, \"total_reward\": -2464.5}\n",
      "{\"step\": 56750, \"performance\": 0.011, \"total_reward\": -2461.75}\n",
      "{\"step\": 57000, \"performance\": -0.13, \"total_reward\": -2494.25}\n",
      "{\"step\": 57250, \"performance\": -0.056, \"total_reward\": -2508.25}\n",
      "{\"step\": 57500, \"performance\": -0.014, \"total_reward\": -2511.75}\n",
      "{\"step\": 57750, \"performance\": -0.087, \"total_reward\": -2533.5}\n",
      "{\"step\": 58000, \"performance\": -0.028, \"total_reward\": -2540.5}\n",
      "{\"step\": 58250, \"performance\": -0.029, \"total_reward\": -2547.75}\n",
      "{\"step\": 58500, \"performance\": -0.042, \"total_reward\": -2558.25}\n",
      "{\"step\": 58750, \"performance\": -0.014, \"total_reward\": -2561.75}\n",
      "{\"step\": 59000, \"performance\": 0.008, \"total_reward\": -2559.75}\n",
      "{\"step\": 59250, \"performance\": -0.058, \"total_reward\": -2574.25}\n",
      "{\"step\": 59500, \"performance\": 0.009, \"total_reward\": -2572.0}\n",
      "{\"step\": 59750, \"performance\": -0.098, \"total_reward\": -2596.5}\n",
      "{\"step\": 60000, \"performance\": 0.01, \"total_reward\": -2594.0}\n",
      "{\"step\": 60250, \"performance\": -0.088, \"total_reward\": -2616.0}\n",
      "{\"step\": 60500, \"performance\": -0.017, \"total_reward\": -2620.25}\n",
      "{\"step\": 60750, \"performance\": -0.056, \"total_reward\": -2634.25}\n",
      "{\"step\": 61000, \"performance\": -0.086, \"total_reward\": -2655.75}\n",
      "{\"step\": 61250, \"performance\": 0.023, \"total_reward\": -2650.0}\n",
      "{\"step\": 61500, \"performance\": -0.049, \"total_reward\": -2662.25}\n",
      "{\"step\": 61750, \"performance\": -0.038, \"total_reward\": -2671.75}\n",
      "{\"step\": 62000, \"performance\": -0.02, \"total_reward\": -2676.75}\n",
      "{\"step\": 62250, \"performance\": -0.073, \"total_reward\": -2695.0}\n",
      "{\"step\": 62500, \"performance\": -0.018, \"total_reward\": -2699.5}\n",
      "{\"step\": 62750, \"performance\": -0.022, \"total_reward\": -2705.0}\n",
      "{\"step\": 63000, \"performance\": -0.04, \"total_reward\": -2715.0}\n",
      "{\"step\": 63250, \"performance\": -0.065, \"total_reward\": -2731.25}\n",
      "{\"step\": 63500, \"performance\": -0.044, \"total_reward\": -2742.25}\n",
      "{\"step\": 63750, \"performance\": -0.059, \"total_reward\": -2757.0}\n",
      "{\"step\": 64000, \"performance\": -0.03, \"total_reward\": -2764.5}\n",
      "{\"step\": 64250, \"performance\": -0.016, \"total_reward\": -2768.5}\n",
      "{\"step\": 64500, \"performance\": -0.069, \"total_reward\": -2785.75}\n",
      "{\"step\": 64750, \"performance\": -0.054, \"total_reward\": -2799.25}\n",
      "{\"step\": 65000, \"performance\": -0.072, \"total_reward\": -2817.25}\n",
      "{\"step\": 65250, \"performance\": 0.033, \"total_reward\": -2809.0}\n",
      "{\"step\": 65500, \"performance\": -0.05, \"total_reward\": -2821.5}\n",
      "{\"step\": 65750, \"performance\": -0.014, \"total_reward\": -2825.0}\n",
      "{\"step\": 66000, \"performance\": -0.103, \"total_reward\": -2850.75}\n",
      "{\"step\": 66250, \"performance\": -0.086, \"total_reward\": -2872.25}\n",
      "{\"step\": 66500, \"performance\": -0.029, \"total_reward\": -2879.5}\n",
      "{\"step\": 66750, \"performance\": -0.085, \"total_reward\": -2900.75}\n",
      "{\"step\": 67000, \"performance\": -0.04, \"total_reward\": -2910.75}\n",
      "{\"step\": 67250, \"performance\": -0.091, \"total_reward\": -2933.5}\n",
      "{\"step\": 67500, \"performance\": -0.004, \"total_reward\": -2934.5}\n",
      "{\"step\": 67750, \"performance\": -0.082, \"total_reward\": -2955.0}\n",
      "{\"step\": 68000, \"performance\": -0.039, \"total_reward\": -2964.75}\n",
      "{\"step\": 68250, \"performance\": 0.027, \"total_reward\": -2958.0}\n",
      "{\"step\": 68500, \"performance\": -0.091, \"total_reward\": -2980.75}\n",
      "{\"step\": 68750, \"performance\": -0.052, \"total_reward\": -2993.75}\n",
      "{\"step\": 69000, \"performance\": 0.021, \"total_reward\": -2988.5}\n",
      "{\"step\": 69250, \"performance\": 0.021, \"total_reward\": -2983.25}\n",
      "{\"step\": 69500, \"performance\": -0.065, \"total_reward\": -2999.5}\n",
      "{\"step\": 69750, \"performance\": 0.052, \"total_reward\": -2986.5}\n",
      "{\"step\": 70000, \"performance\": -0.033, \"total_reward\": -2994.75}\n",
      "{\"step\": 70250, \"performance\": 0.012, \"total_reward\": -2991.75}\n",
      "{\"step\": 70500, \"performance\": -0.012, \"total_reward\": -2994.75}\n",
      "{\"step\": 70750, \"performance\": 0.014, \"total_reward\": -2991.25}\n",
      "{\"step\": 71000, \"performance\": -0.052, \"total_reward\": -3004.25}\n",
      "{\"step\": 71250, \"performance\": -0.088, \"total_reward\": -3026.25}\n",
      "{\"step\": 71500, \"performance\": -0.062, \"total_reward\": -3041.75}\n",
      "{\"step\": 71750, \"performance\": -0.02, \"total_reward\": -3046.75}\n",
      "{\"step\": 72000, \"performance\": -0.079, \"total_reward\": -3066.5}\n",
      "{\"step\": 72250, \"performance\": -0.069, \"total_reward\": -3083.75}\n",
      "{\"step\": 72500, \"performance\": -0.038, \"total_reward\": -3093.25}\n",
      "{\"step\": 72750, \"performance\": -0.06, \"total_reward\": -3108.25}\n",
      "{\"step\": 73000, \"performance\": -0.049, \"total_reward\": -3120.5}\n",
      "{\"step\": 73250, \"performance\": 0.005, \"total_reward\": -3119.25}\n",
      "{\"step\": 73500, \"performance\": -0.078, \"total_reward\": -3138.75}\n",
      "{\"step\": 73750, \"performance\": -0.027, \"total_reward\": -3145.5}\n",
      "{\"step\": 74000, \"performance\": -0.102, \"total_reward\": -3171.0}\n",
      "{\"step\": 74250, \"performance\": -0.068, \"total_reward\": -3188.0}\n",
      "{\"step\": 74500, \"performance\": -0.032, \"total_reward\": -3196.0}\n",
      "{\"step\": 74750, \"performance\": -0.029, \"total_reward\": -3203.25}\n",
      "{\"step\": 75000, \"performance\": -0.099, \"total_reward\": -3228.0}\n",
      "{\"step\": 75250, \"performance\": 0.011, \"total_reward\": -3225.25}\n",
      "{\"step\": 75500, \"performance\": 0.021, \"total_reward\": -3220.0}\n",
      "{\"step\": 75750, \"performance\": -0.03, \"total_reward\": -3227.5}\n",
      "{\"step\": 76000, \"performance\": -0.072, \"total_reward\": -3245.5}\n",
      "{\"step\": 76250, \"performance\": -0.059, \"total_reward\": -3260.25}\n",
      "{\"step\": 76500, \"performance\": -0.083, \"total_reward\": -3281.0}\n",
      "{\"step\": 76750, \"performance\": -0.089, \"total_reward\": -3303.25}\n",
      "{\"step\": 77000, \"performance\": -0.011, \"total_reward\": -3306.0}\n",
      "{\"step\": 77250, \"performance\": -0.05, \"total_reward\": -3318.5}\n",
      "{\"step\": 77500, \"performance\": -0.03, \"total_reward\": -3326.0}\n",
      "{\"step\": 77750, \"performance\": -0.008, \"total_reward\": -3328.0}\n",
      "{\"step\": 78000, \"performance\": 0.058, \"total_reward\": -3313.5}\n",
      "{\"step\": 78250, \"performance\": -0.016, \"total_reward\": -3317.5}\n",
      "{\"step\": 78500, \"performance\": -0.067, \"total_reward\": -3334.25}\n",
      "{\"step\": 78750, \"performance\": -0.034, \"total_reward\": -3342.75}\n",
      "{\"step\": 79000, \"performance\": -0.052, \"total_reward\": -3355.75}\n",
      "{\"step\": 79250, \"performance\": -0.018, \"total_reward\": -3360.25}\n",
      "{\"step\": 79500, \"performance\": -0.014, \"total_reward\": -3363.75}\n",
      "{\"step\": 79750, \"performance\": -0.021, \"total_reward\": -3369.0}\n",
      "{\"step\": 80000, \"performance\": -0.092, \"total_reward\": -3392.0}\n",
      "{\"step\": 80250, \"performance\": -0.026, \"total_reward\": -3398.5}\n",
      "{\"step\": 80500, \"performance\": 0.003, \"total_reward\": -3397.75}\n",
      "{\"step\": 80750, \"performance\": -0.027, \"total_reward\": -3404.5}\n",
      "{\"step\": 81000, \"performance\": -0.027, \"total_reward\": -3411.25}\n",
      "{\"step\": 81250, \"performance\": 0.019, \"total_reward\": -3406.5}\n",
      "{\"step\": 81500, \"performance\": 0.023, \"total_reward\": -3400.75}\n",
      "{\"step\": 81750, \"performance\": 0.01, \"total_reward\": -3398.25}\n",
      "{\"step\": 82000, \"performance\": -0.043, \"total_reward\": -3409.0}\n",
      "{\"step\": 82250, \"performance\": -0.026, \"total_reward\": -3415.5}\n",
      "{\"step\": 82500, \"performance\": -0.026, \"total_reward\": -3422.0}\n",
      "{\"step\": 82750, \"performance\": -0.068, \"total_reward\": -3439.0}\n",
      "{\"step\": 83000, \"performance\": -0.005, \"total_reward\": -3440.25}\n",
      "{\"step\": 83250, \"performance\": 0.007, \"total_reward\": -3438.5}\n",
      "{\"step\": 83500, \"performance\": -0.002, \"total_reward\": -3439.0}\n",
      "{\"step\": 83750, \"performance\": 0.021, \"total_reward\": -3433.75}\n",
      "{\"step\": 84000, \"performance\": -0.04, \"total_reward\": -3443.75}\n",
      "{\"step\": 84250, \"performance\": -0.101, \"total_reward\": -3469.0}\n",
      "{\"step\": 84500, \"performance\": -0.027, \"total_reward\": -3475.75}\n",
      "{\"step\": 84750, \"performance\": -0.047, \"total_reward\": -3487.5}\n",
      "{\"step\": 85000, \"performance\": -0.026, \"total_reward\": -3494.0}\n",
      "{\"step\": 85250, \"performance\": 0.057, \"total_reward\": -3479.75}\n",
      "{\"step\": 85500, \"performance\": -0.08, \"total_reward\": -3499.75}\n",
      "{\"step\": 85750, \"performance\": -0.046, \"total_reward\": -3511.25}\n",
      "{\"step\": 86000, \"performance\": -0.002, \"total_reward\": -3511.75}\n",
      "{\"step\": 86250, \"performance\": -0.059, \"total_reward\": -3526.5}\n",
      "{\"step\": 86500, \"performance\": -0.036, \"total_reward\": -3535.5}\n",
      "{\"step\": 86750, \"performance\": -0.014, \"total_reward\": -3539.0}\n",
      "{\"step\": 87000, \"performance\": -0.061, \"total_reward\": -3554.25}\n",
      "{\"step\": 87250, \"performance\": -0.028, \"total_reward\": -3561.25}\n",
      "{\"step\": 87500, \"performance\": -0.089, \"total_reward\": -3583.5}\n",
      "{\"step\": 87750, \"performance\": -0.008, \"total_reward\": -3585.5}\n",
      "{\"step\": 88000, \"performance\": -0.088, \"total_reward\": -3607.5}\n",
      "{\"step\": 88250, \"performance\": -0.062, \"total_reward\": -3623.0}\n",
      "{\"step\": 88500, \"performance\": -0.086, \"total_reward\": -3644.5}\n",
      "{\"step\": 88750, \"performance\": -0.146, \"total_reward\": -3681.0}\n",
      "{\"step\": 89000, \"performance\": -0.104, \"total_reward\": -3707.0}\n",
      "{\"step\": 89250, \"performance\": -0.097, \"total_reward\": -3731.25}\n",
      "{\"step\": 89500, \"performance\": 0.004, \"total_reward\": -3730.25}\n",
      "{\"step\": 89750, \"performance\": -0.074, \"total_reward\": -3748.75}\n",
      "{\"step\": 90000, \"performance\": -0.061, \"total_reward\": -3764.0}\n",
      "{\"step\": 90250, \"performance\": -0.074, \"total_reward\": -3782.5}\n",
      "{\"step\": 90500, \"performance\": -0.102, \"total_reward\": -3808.0}\n",
      "{\"step\": 90750, \"performance\": -0.036, \"total_reward\": -3817.0}\n",
      "{\"step\": 91000, \"performance\": -0.025, \"total_reward\": -3823.25}\n",
      "{\"step\": 91250, \"performance\": -0.06, \"total_reward\": -3838.25}\n",
      "{\"step\": 91500, \"performance\": -0.02, \"total_reward\": -3843.25}\n",
      "{\"step\": 91750, \"performance\": -0.049, \"total_reward\": -3855.5}\n",
      "{\"step\": 92000, \"performance\": -0.03, \"total_reward\": -3863.0}\n",
      "{\"step\": 92250, \"performance\": -0.058, \"total_reward\": -3877.5}\n",
      "{\"step\": 92500, \"performance\": -0.047, \"total_reward\": -3889.25}\n",
      "{\"step\": 92750, \"performance\": -0.093, \"total_reward\": -3912.5}\n",
      "{\"step\": 93000, \"performance\": -0.079, \"total_reward\": -3932.25}\n",
      "{\"step\": 93250, \"performance\": -0.138, \"total_reward\": -3966.75}\n",
      "{\"step\": 93500, \"performance\": -0.106, \"total_reward\": -3993.25}\n",
      "{\"step\": 93750, \"performance\": -0.11, \"total_reward\": -4020.75}\n",
      "{\"step\": 94000, \"performance\": -0.083, \"total_reward\": -4041.5}\n",
      "{\"step\": 94250, \"performance\": -0.108, \"total_reward\": -4068.5}\n",
      "{\"step\": 94500, \"performance\": -0.135, \"total_reward\": -4102.25}\n",
      "{\"step\": 94750, \"performance\": -0.113, \"total_reward\": -4130.5}\n",
      "{\"step\": 95000, \"performance\": -0.097, \"total_reward\": -4154.75}\n",
      "{\"step\": 95250, \"performance\": -0.056, \"total_reward\": -4168.75}\n",
      "{\"step\": 95500, \"performance\": -0.066, \"total_reward\": -4185.25}\n",
      "{\"step\": 95750, \"performance\": -0.101, \"total_reward\": -4210.5}\n",
      "{\"step\": 96000, \"performance\": -0.091, \"total_reward\": -4233.25}\n",
      "{\"step\": 96250, \"performance\": -0.114, \"total_reward\": -4261.75}\n",
      "{\"step\": 96500, \"performance\": -0.103, \"total_reward\": -4287.5}\n",
      "{\"step\": 96750, \"performance\": -0.11, \"total_reward\": -4315.0}\n",
      "{\"step\": 97000, \"performance\": -0.091, \"total_reward\": -4337.75}\n",
      "{\"step\": 97250, \"performance\": -0.112, \"total_reward\": -4365.75}\n",
      "{\"step\": 97500, \"performance\": -0.098, \"total_reward\": -4390.25}\n",
      "{\"step\": 97750, \"performance\": -0.088, \"total_reward\": -4412.25}\n",
      "{\"step\": 98000, \"performance\": -0.096, \"total_reward\": -4436.25}\n",
      "{\"step\": 98250, \"performance\": -0.084, \"total_reward\": -4457.25}\n",
      "{\"step\": 98500, \"performance\": -0.091, \"total_reward\": -4480.0}\n",
      "{\"step\": 98750, \"performance\": -0.041, \"total_reward\": -4490.25}\n",
      "{\"step\": 99000, \"performance\": -0.009, \"total_reward\": -4492.5}\n",
      "{\"step\": 99250, \"performance\": 0.018, \"total_reward\": -4488.0}\n",
      "{\"step\": 99500, \"performance\": -0.026, \"total_reward\": -4494.5}\n",
      "{\"step\": 99750, \"performance\": -0.098, \"total_reward\": -4519.0}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "iters = 100000\n",
    "agent = Agent(learning_rate=1e-4, discount=0.1, iterations=iters)\n",
    "\n",
    "# setup simulation\n",
    "environment = EnvironmentSimulator(labels=y)\n",
    "# environment.reset()\n",
    "total_reward = 0 # Score keeping\n",
    "last_total = 0\n",
    "\n",
    "train_inds = X.index[:round(0.7 * X.shape[0])]\n",
    "test_inds = X.index[round(0.7 * X.shape[0]):]\n",
    "\n",
    "# train\n",
    "for step in range(iters):\n",
    "    old_state = np.random.choice(train_inds, 1)[0] # Store current state\n",
    "    action = agent.get_next_action(old_state) # Query agent for the next action\n",
    "    reward = environment.take_action(state=old_state, action=action) # Take action, get new state and reward\n",
    "    new_state = np.random.choice(train_inds, 1)[0]\n",
    "    agent.update(old_state, new_state, action, reward) # Let the agent update internals\n",
    "\n",
    "    total_reward += reward # Keep score\n",
    "    if step % 250 == 0: # Print out metadata every 250th iteration\n",
    "        performance = (total_reward - last_total) / 250.0\n",
    "        print(json.dumps({'step': step, 'performance': performance, 'total_reward': total_reward}))\n",
    "        last_total = total_reward\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for ind in test_inds:\n",
    "    y_pred.append(agent.get_next_action(ind))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "y_test = y.iloc[test_inds]\n",
    "y_train = y.iloc[train_inds]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score :  0.8998476755056\n",
      "F1 score :  0.825348957833034\n",
      "ROC-AUC:  0.8183675505694373\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "print(\"Accuracy score : \", accuracy_score(y_train, y_pred))\n",
    "print(\"F1 score : \", f1_score(y_train, y_pred))\n",
    "# print(\"Precision score : \", precision_score(y_train, y_pred))\n",
    "# print(\"Recall score : \", recall_score(y_train, y_pred))\n",
    "print('ROC-AUC: ', roc_auc_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}